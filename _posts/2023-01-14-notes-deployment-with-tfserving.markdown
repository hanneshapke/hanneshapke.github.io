---
layout: post
title:  Notes on deploying models with TFServing
description: A collection of useful links with information about the inner working of TFServing
date:   2023-01-12 12:00:00 -0700
image:  '/images/pietro-jeng-n6B49lTx7NM-unsplash.jpg'
tags:   [machine learning, mlops, deployments]
---

I think TFServing is a gold standard of deploying deep learning models. It is lean, memory efficient, and supports a number of non-TensorFlow frameworks like JAX, scikit learn or XGBoost.

Here are some notes I constantly refer for details beyong the Google documentation:
* [Deploying production ML models with TensorFlow Serving](https://docs.google.com/presentation/d/1yx4oH94R6BNBwiNZHLHHlEUzBLk33ZwX7L6TM4MQ3HM/)
* [Scaling TensorFlow to 300 million predictions per second](https://arxiv.org/pdf/2109.09541.pdf)
